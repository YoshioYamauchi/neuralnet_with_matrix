{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Optimized with Matrix Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I implemented a neural network optimized with matrix calculation. Though neural networks can be implemented with many 'for' loops, which may be easy to read, the traininig time may become longer because we need at leas 3 'for' loops. When we implement such a network with 30 epochs, 50000 mnist training data, and mini_batch size 10, the network has to implement 150000 loops in tota.\n",
    "\n",
    "This is quite expensive, so I implemented a neural network which uses two for loops and some matrix calculation. I used matrix caculation on update over mini-batch example. The update shares the same weights and biases in the network, so we can conduct the update simultaneously over the examples in the mini-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, update over mini-batch is conducted using one 'for' loop. We iterate over the examples in the mini-batch. In each loop, we calculate the gradient of the cost function with respect to weight matrix nad biases of the neurons. \n",
    "\n",
    "Suppose that ${C_x}_i$ is the cost calculated over a specific example $x_i$ in the mini_batch. Althoug we want to calculate the gradient of the cost $C_{all}$ over the entier training examples, it would be quite expensive computation. So we estimate the gradient with some randomely chosen examples. Then out estimate of the gradient is: \n",
    "$$\n",
    "\\widehat{ \\nabla_{w} C_{all} } = \\frac{1}{m}\\sum_{i=0}^{m}\\nabla_{w}C_{xi} \n",
    "$$\n",
    "$$\n",
    "\\widehat{ \\nabla_{b} C_{all} } = \\frac{1}{m}\\sum_{i=0}^{m}\\nabla_{b}C_{xi} \n",
    "$$\n",
    "where m is the mini-batch size. Each gradient of $C_x$ with respect to $\\mathbf{w}$ and b can be calculated using backpropagation. Suppose that we use the corss entropy as the cost function and that add a L2 regularization term. Then the cost over the entire training examples is representes as:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "C_{all} = \\sum_{x}^{}C_{0x} + \\frac{\\lambda}{2n}\\sum_{\\mathbf{w}}^{}\\mathbf{w}^2\\\\\n",
    "\\text{where }C_{0x} = \\sum_{j}^{}[ y_j log a_j^L + (1-y_j)log( 1 - a_j^L )]\n",
    "\\end{equation}\n",
    "$$\n",
    "$a_j^L$ is the jth activation output from the output layer. So we update $\\mathbf{w}$ as :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{w} &\\leftarrow \\mathbf{w} - \\eta \\nabla_{w}C_{all} \\\\\n",
    "&= \\mathbf{w} - \\eta ( \\nabla_{w}C_{0all} + \\frac{\\lambda}{n}\\mathbf{w} )\\\\\n",
    "&= ( 1 - \\frac{\\eta\\lambda}{n} )\\mathbf{w} - \\eta\\nabla_{w}C_{0all}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Because we can not clculate $C_{0all}$, we estimate it over the examples in a mini_batch.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{w} &= ( 1 - \\frac{\\eta\\lambda}{n} )\\mathbf{w} - \\eta\\frac{1}{m}\\sum_{i=0}^{m}\\nabla_{w}C_{0xi} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and biases are also updated as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{w} &\\leftarrow \\mathbf{w} - \\frac{\\eta}{m}\\sum_{x}^{}\\nabla_{b}C_{0xi}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "But we cannot directly drive these gradient of the cost function, so we use the back propagation algorithm to get them. Let $\\mathbf{z}^l$ and $\\mathbf{a}^l$ be the inputs to the neurons in the lth layer and its sigmoid activation respectively. And we also define $\\mathbf{\\delta}^l$ as the gradient of the const function with respect to $\\mathbf{z}^l$\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{z}^{l+1} = \\mathbf{w}^{l}\\mathbf{z}^{l} + \\mathbf{b}^{l}\\\\\n",
    "\\mathbf{a}^{l} = \\sigma ( \\mathbf{z}^{l} )\\\\\n",
    "\\mathbf{\\delta}^{l} = \\nabla_{z^l}C_{0xi}\n",
    "\\end{equation}\n",
    "$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python code\n",
    "The python code is composed with three separete file. \n",
    "The first code $\\textbf{network_optimized.py}$ defines a class called 'Network'. This class holds basic parameters that defines the structure of the network. We give the number of neurons in the hidden layer when we instanciate it. The main function for training is 'SGD'. We give the function some hyper parameters specifying how the network learns, such as learing late $\\eta$, the number of epochs, mini-bach size, and lambda $\\lambda$. $\\lambda$ defines how much the regularization term affects the way the networks learns. When it is large, it will be more beneficial, when we minimize the cost function, to make the weights small rather than minimize the non-regularized part of the cost function. And hence, the network will have strong generality. Conversely, when $\\lambda = 0$, the network will behave as a non-regularized one and the weights may become larger as the training proceeds.\n",
    "\n",
    "SGD function calls 'update_mini_batch' function to update $\\mathbf{w}$ and $b$ over an mini-batch. We use matrix calculation to derive the estimated gradient of the cost function rather than iterate over the examples in the mini-batch. The update function takes mini-batch examples minibatch_x and minibatch_y, which are matrices. The number of rows is the mini-batch size. minibatch_x and minibatch_y have 784 and 10 columns respectively.\n",
    "\n",
    "We first feedforward through the network to calculate the $\\mathbf{z}$ vectors and its activations. Note that we calculate them simultaneously over all the exmples in the mini-batch using matrix calculation. Afther the feedforward, we conduct the back-propagation. Also in the back-propagation, $\\delta$ vector is expanded to a matrixs, where jth column in the matrix represents $\\delta$ of the jth example in the mini-batch. $\\mathbf{z}$ vecotors and its activations also have the same structure.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{z} &= ( \\mathbf{z_1}, \\dotsc, \\mathbf{z_m} ) \\\\\n",
    "\\mathbf{a} &= \\sigma( \\mathbf{z} )\n",
    "\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Calculating the gradient of the cost function with respect to $\\mathbf{w}$ is a little complicated. Usually, when we do that with 'for' loop, we first calculate the gradients for each example in the mini-batch and then take its average. However, with the matrix compulation we conduct the two operations simultraneously in a single line of code.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Last Updated: March 6, 2018\n",
    "# Note:\n",
    "# the vectorized version of the network2. Only one hidden layer is allowed,\n",
    "# and we use only cross entropy cost.\n",
    "\n",
    "\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "class Network ( object ) :\n",
    "    def __init__ ( self, num_hidden_neuron ) :\n",
    "        # the number of neurons in the hidded layer\n",
    "        self.num_hidden_neuron = num_hidden_neuron\n",
    "        # the definistions of the biases vecotors and weights matrices.\n",
    "        self.biases_hidlayer = np.random.randn ( num_hidden_neuron )\n",
    "        self.biases_outlayer = np.random.randn ( 10 )\n",
    "        self.weights_hidlayer = \\\n",
    "          np.random.randn ( num_hidden_neuron, 784 ) / 10.\n",
    "        self.weights_outlayer = \\\n",
    "          np.random.randn ( 10, num_hidden_neuron ) / num_hidden_neuron\n",
    "\n",
    "\n",
    "    def SGD ( self,\n",
    "              # when whole_training_data is the whole mnist training data,\n",
    "              # whole_training_data [ 0 ] <-shape( 50000, 784 )\n",
    "              # whole_training_data [ 1 ] <-shape( 50000, 10 )\n",
    "              whole_training_data,\n",
    "              epochs,\n",
    "              mini_batch_size, # 10 in demo.py\n",
    "              eta, # 0.3 in demp.py\n",
    "              lmbda = 0.0, # 3.0 in demo.py\n",
    "              # evaluation data is either validation data or test data.\n",
    "              evaluation_data = None,\n",
    "              evaluate_on_training_data = False,\n",
    "              evaluate_on_evaluation_data = False ) :\n",
    "\n",
    "        if evaluation_data : n_data = len ( evaluation_data [ 0 ] ) # 10000\n",
    "        whole_training_data_size = len ( whole_training_data [ 0 ] ) # 50000\n",
    "        training_cost, training_accuracy = [], []\n",
    "        evaluation_cost, evaluation_accuracy = [], []\n",
    "        index = np.arange ( whole_training_data_size )\n",
    "        for epc in xrange ( epochs ) :\n",
    "            print \"Epoch %d: Start\" % epc\n",
    "            np.random.shuffle ( index )\n",
    "            whole_training_x = whole_training_data [ 0 ] [ index, : ]\n",
    "            whole_training_y = whole_training_data [ 1 ] [ index, : ]\n",
    "            for j in xrange ( 0, whole_training_data_size, mini_batch_size  ) :\n",
    "                minibatch_x = \\\n",
    "                  whole_training_x [ j : j + mini_batch_size, : ]\n",
    "                minibatch_y = \\\n",
    "                  whole_training_y [ j : j + mini_batch_size, : ]\n",
    "                self.update_mini_batch ( minibatch_x, minibatch_y, eta,\n",
    "                                    lmbda, whole_training_data_size )\n",
    "\n",
    "            print \"Epoch %d: training complete\" % epc\n",
    "            if evaluate_on_training_data :\n",
    "                accuracy, cost = self.evaluate ( whole_training_data [ 0 ],\n",
    "                                                 whole_training_data [ 1 ],\n",
    "                                                 lmbda )\n",
    "                training_cost.append ( cost )\n",
    "                training_accuracy.append ( accuracy )\n",
    "            if evaluate_on_evaluation_data and evaluation_data :\n",
    "                accuracy, cost = self.evaluate ( evaluation_data [ 0 ],\n",
    "                                                 evaluation_data [ 1 ],\n",
    "                                                 lmbda )\n",
    "                evaluation_cost.append ( cost )\n",
    "                evaluation_accuracy.append ( accuracy )\n",
    "\n",
    "        return training_cost, training_accuracy,\\\n",
    "               evaluation_cost, evaluation_accuracy\n",
    "\n",
    "    def update_mini_batch ( self, minibatch_x, minibatch_y, eta, lmbda,\n",
    "                            whole_training_data_size ) :\n",
    "        mini_batch_size = minibatch_x.shape [ 0 ]\n",
    "        ## feedforward\n",
    "        a_inlayer = minibatch_x.T\n",
    "        z_hidlayer = np.dot ( self.weights_hidlayer, a_inlayer ) + \\\n",
    "                     self.biases_hidlayer [ :, None ]\n",
    "        a_hidlayer = sigmoid ( z_hidlayer )\n",
    "        z_outlayer = np.dot ( self.weights_outlayer, a_hidlayer ) + \\\n",
    "                     self.biases_outlayer [ :, None ]\n",
    "        a_outlayer = sigmoid ( z_outlayer )\n",
    "\n",
    "        ## back propagate\n",
    "        delta_outlayer = a_outlayer - minibatch_y.T\n",
    "        biases_outlayer_grad = delta_outlayer.mean ( axis = 1 )\n",
    "        weights_outlayer_grad = \\\n",
    "          np.dot ( delta_outlayer, a_hidlayer.T ) / mini_batch_size\n",
    "        delta_hidlayer = \\\n",
    "          sigmoid_prime ( z_hidlayer ) * \\\n",
    "          np.dot ( self.weights_outlayer.T, delta_outlayer )\n",
    "        biases_hidlayer_grad = delta_hidlayer.mean ( axis = 1 )\n",
    "        weights_hidlayer_grad = \\\n",
    "          np.dot ( delta_hidlayer, a_inlayer.T ) / mini_batch_size\n",
    "\n",
    "        ## update biases\n",
    "        self.biases_outlayer = \\\n",
    "          self.biases_outlayer - eta * biases_outlayer_grad\n",
    "        self.biaess_hidlayer = \\\n",
    "          self.biases_hidlayer - eta * biases_hidlayer_grad\n",
    "        # update weights\n",
    "        self.weights_outlayer = \\\n",
    "          ( 1. - eta * lmbda / whole_training_data_size ) * \\\n",
    "          self.weights_outlayer - eta * weights_outlayer_grad\n",
    "        self.weights_hidlayer = \\\n",
    "          ( 1. - eta * lmbda / whole_training_data_size ) * \\\n",
    "          self.weights_hidlayer - eta * weights_hidlayer_grad\n",
    "\n",
    "    def evaluate ( self, data_x, data_y, lmbda ) :\n",
    "        '''\n",
    "        calcuate accuracy and cost on the given data. the format of data_x\n",
    "        and data_y is the same as the training_data. The cost function is\n",
    "        the coross entropy cost\n",
    "        '''\n",
    "        num_data = data_x.shape [ 0 ]\n",
    "\n",
    "        ## feedforward\n",
    "        a_inlayer = data_x.T\n",
    "        z_hidlayer = np.dot ( self.weights_hidlayer, a_inlayer ) + \\\n",
    "                     self.biases_hidlayer [ :, None ]\n",
    "        a_hidlayer = sigmoid ( z_hidlayer )\n",
    "        z_outlayer = np.dot ( self.weights_outlayer, a_hidlayer ) + \\\n",
    "                     self.biases_outlayer [ :, None ]\n",
    "        a_outlayer = sigmoid ( z_outlayer )\n",
    "\n",
    "        ## calcualte accuracy\n",
    "        network_output = a_outlayer.argmax ( axis = 0 )\n",
    "        desired_output = data_y.argmax ( axis = 1 )\n",
    "        succeeds = ( network_output == desired_output ).sum ()\n",
    "        accuracy = ( succeeds * 100.0 ) / num_data\n",
    "\n",
    "        # calculate cost\n",
    "        cost_C0 = np.nan_to_num \\\n",
    "          ( - data_y.T * np.log ( a_outlayer ) - \\\n",
    "          ( 1 - data_y.T ) * np.log ( 1 - a_outlayer ) ).sum () / num_data\n",
    "        cost_regularization_term = \\\n",
    "          0.5 * lmbda / num_data *\\\n",
    "          ( np.linalg.norm ( self.weights_hidlayer ) ** 2 +\\\n",
    "            np.linalg.norm ( self.weights_outlayer ) ** 2 )\n",
    "        cost = cost_C0 + cost_regularization_term\n",
    "\n",
    "        return accuracy, cost\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid ( x ) :\n",
    "    return 1./ ( 1. + np.exp ( -x ) )\n",
    "\n",
    "def sigmoid_prime ( x ) :\n",
    "    # return sigmoid ( x ) * ( 1 - sigmoid ( x ) )\n",
    "    return 1./ ( 2. + np.exp ( x ) + np.exp ( -x ) )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
